<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta charset="utf-8">
    <meta name="citation_title" content="Augmenting Long-term Memory">
    <meta name="citation_author" content="Nielsen, Michael">
    <meta name="citation_publication_date" content="2018">
    <meta name="citation_fulltext_html_url" content="http://augmentingcognition.com/ltm.html">
    <meta name="citation_fulltext_world_readable" content="">

    <title>Augmenting Long-term Memory</title>
    <link rel="stylesheet" type="text/css" href="style.css">
  </head>

  <body>
    <div id="header">
      <h1>Augmenting Long-term Memory</h1>
      <p>
	<a href="http://michaelnielsen.org">Michael Nielsen</a> &nbsp;
	| &nbsp; <a href="https://ycr.org">Y Combinator Research</a> &nbsp;
	| &nbsp; July 2018
      </p>
    </div>

    <div id="container">
      
      <span id="sidebar">
  <a href="index.html">How to Augment Human Cognition?</a><br>
  How to Invent New Primitives of Thought<br>
  <a href="ltm.html">Augmenting Long-term Memory</a><br>
  Toward a Young Lady's Illustrated Primer<br>
  Using Personal Memory Systems to Understand Mathematics<br>
  The Humane Use of Augmentation<br>
  <br>
  <strong>Resources</strong><br>
  <a href="https://twitter.com/michael_nielsen">Michael Nielsen on Twitter</a><br>
  <a href="http://eepurl.com/0Xxjb">Michael Nielsen's project announcement mailing list</a><br>
  <a href="http://cognitivemedium.com">cognitivemedium.com</a><br>
  <hr>
  <a href="http://michaelnielsen.org"><img src="assets/Michael_Nielsen_Web_Small.jpg" width="160px" style="border-style: none;"/></a><br>
  By <a href="http://michaelnielsen.org">Michael Nielsen</a></br>

</span>

      
    <p>
      One day in the mid-1920s, a Moscow newspaper reporter named
      Solomon Shereshevsky entered the laboratory of the psychologist
      Alexander Luria. Shereshevsky explained to Luria that his boss
      at the newspaper was surprised he didn't need to take any notes,
      but somehow still remembered all he was told, and had suggested
      he get his memory checked.
    </p>

    <p>
      Luria began testing Shereshevsky's memory. He began with simple
      tests, short strings of words and of numbers. Shereshevsky
      remembered these with ease, and so Luria gradually increased the
      length of the strings. But no matter how long they got,
      Shereshevsky could recite them back.  Fascinated, Luria went on
      to study Shereshevsky's memory for the next 30 years.  In a book
      summing up his research*<span class="marginnote">* Alexander
      Luria, &ldquo;The Mind of a Mnemonist&rdquo;, Harvard University
      Press (1968).</span>, Luria reported that:
    </p>

    <blockquote>
      [I]t appeared that there was no limit either to
      the <em>capacity</em> of S.'s memory or to the <em>durability of
      the traces he retained</em>. Experiments indicated that he had
      no difficulty reproducing any lengthy series of words whatever,
      even though these had originally been presented to him a week, a
      month, a year, or even many years earlier. In fact, some of
      these experiments designed to test his retention were performed
      (without his being given any warning) fifteen or sixteen years
      after the session in which he had originally recalled the
      words. Yet invariably they were successful.
    </blockquote>
      
    <p>
      Such stories are fascinating. Memory is fundamental to our
      thinking, and there is something seductive about the notion of
      having a perfect memory.  At the same time, many people feel
      ambivalent about their own memory. I've often heard people say
      &ldquo;I don't have a very good memory&rdquo;, sometimes
      sheepishly, sometimes apologetically, sometimes even defiantly.
    </p>

    <p>
      Given how central memory is to our thinking, it's natural to ask
      whether computers can somehow be used as tools to help improve
      our memory. This question turns out to be highly generative of
      good new ideas: indeed, pursuing it has led to many of the most
      important vision documents in the history of computing. One
      early example was Vannevar Bush's 1945
      proposal*<span class="marginnote">* Vannevar
      Bush, <a href="https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/">As
      We May Think</a>, The Atlantic (1945).</span> for a mechanical
      memory extender, the memex. Bush wrote:
    </p>

    <blockquote>
      A memex is a device in which an individual stores all his books,
      records, and communications, and which is mechanized so that it
      may be consulted with exceeding speed and flexibility. It is an
      enlarged intimate supplement to his memory.
    </blockquote>

    <p>
      The memex vision inspired many later computer pioneers,
      including Douglas Engelbart's ideas about augmentation, Ted
      Nelson's ideas about hypertext, and, indirectly, Tim
      Berners-Lee's conception of the world wide
      web*<span class="marginnote">* See, for example: Douglas
      Engelbart, <a href="assets/Engelbart1962.pdf">Augmenting Human
      Intellect</a> (1962); Ted
      Nelson, <a href="https://dl.acm.org/citation.cfm?id=806036">Complex
      information processing: a file structure for the complex, the
      changing and the indeterminate</a> (1965); and Tim
      Berners-Lee, <a href="https://w3.org/History/1989/proposal.html">Information
      Management: a Proposal</a> (1989).</span>. In his proposal for
      the web, Berners-Lee describes the need for his employer (the
      particle physics organization CERN) to develop a kind of
      collective institutional memory,
    </p>

    <blockquote>
      a pool of information to develop which could grow and evolve
      with the organization and the projects it describes.
    </blockquote>


    <p>
      These are just a few of the many attempts made to use computers
      to augment human memory. From the memex to the web to wikis to
      <a href="https://orgmode.org/">org-mode</a>
      to <a href="https://en.wikipedia.org/wiki/Project_Xanadu">Project
      Xanadu</a> to attempts
      to <a href="https://users.speakeasy.net/~lion/nb/book.pdf">make
      a map of every thought a person thinks</a>: the augmentation of
      memory has been an extremely generative vision for computing.
    </p>

      <p>
	In this essay we investigate personal memory systems, that is,
	systems designed to improve the long-term memory of a single
	person. In the first part of the essay I describe my own
	experience using such a system, named Anki. As we'll see, Anki
	can be used to remember almost anything! That is, Anki makes
	memory a <em>choice</em>, rather than a haphazard event, to be
	left to chance. I'll discuss how to use Anki to understand
	research papers, books, and much else. And I'll describe
	numerous patterns and anti-patterns for Anki use. While Anki
	is an extremely simple program, it's possible to develop
	virtuoso skill using Anki, a skill aimed at understanding
	complex material, not just memorizing simple facts. The first
	part of the essay can be thought of as a how-to guide aimed at
	helping develop such a skill.
      </p>

      <p>
	The second part of the essay discusses the principles
	underlying personal memory systems in general. Many people
	treat memory somewhat disparagingly as a cognitive skill: for
	instance, people will often talk of &ldquo;rote memory&rdquo;
	as though it's somehow inferior to more advanced kinds of
	understanding. I'll argue against this point of view, and in
	favor of memory as central to problem solving and creative
	skills. Also in this second part, we'll review the role of
	cognitive science in building personal memory systems, and
	speculate briefly on the development of novel personal memory
	systems. One idea will be described in more detail in a future
	essay: <a href="taylip.html">Toward a Young Lady's Illustrated
	Primer</a>.
      </p>

      <p>
	The essay is unusual in style. It's not a conventional
	cognitive science paper, a study of human memory and how it
	works. Nor is it a systems design paper, though prototyping
	systems is my own main interest. Rather, the essay is a
	distillation of my informal, <em>ad hoc</em> observations and
	rules of thumb about how certain personal memory systems
	work. I wanted to understand those as preparation for building
	systems of my own. And as I collected them it seemed possible
	they may be of interest to others as well.
      </p>

      <p>
	To conclude this introduction, a couple of words on what the
	essay is not. It will not describe attempts to use
	pharmaceuticals to improve memory, nor possible future
	brain-computer interfaces to augment memory. Those seem likely
	to eventually become important subjects, and need a separate
	treatment. But, as we shall see, there are already very
	powerful ideas about personal memory systems based solely on
	the structuring and presentation of information.
      </p>

      <h2>Part I: How to remember almost anything: the Anki
      system</h2>

    <p>
      I'll begin with an account of my own experience with the
      personal memory
      system <a href="https://apps.ankiweb.net/">Anki</a>*<span class="marginnote">*
      I've no affiliation at all with Anki. Other similar systems
      include <a href="https://mnemosyne-proj.org">Mnemosyne</a>
      and <a href="https://supermemo.com">SuperMemo</a>. My limited
      use suggests Mnemosyne is very similar to Anki.  SuperMemo runs
      only on Windows, and I haven't had an opportunity to use it,
      though I have been influenced by essays on
      the <a href="https://supermemo.com">SuperMemo
      website</a>. </span>. The material is, as mentioned above, quite
      personal, a collection of my own observations and informal rules
      of thumb.  Those rules of thumb may not apply to others; indeed,
      I may be mistaken about how well they apply to me. It's
      certainly not a properly controlled study of Anki usage!  Still,
      I believe there is value in collecting such personal
      experiences. Note that I am not an expert on the cognitive
      science of memory, and I'd appreciate corrections to any errors
      or misconceptions.
    </p>

      <p>
	Before we begin, a warning. Systems such as Anki are, in my
	opinion, powerful, and the early parts of the essay will
	sounds like an enthusiastic sales pitch.  It's certainly true
	that Anki has become an important part of my life. But it's
	also required a great deal of work to learn to use well. As
	we'll discuss, it required several failed attempts to make it
	worth using. So it's not a panacea; later, we'll discuss some
	shortcomings and possible improvements. That said, I do
	believe the underlying principles are powerful and important
	and deserve serious consideration.
      </p>

      <p>
	At first glance, Anki seems nothing more than a computerized
      flashcard program. You enter a question:
    </p>

   <img src="assets/Anki_question.png">

    <p>
      And a corresponding answer:
    </p>

    <img src="assets/Anki_answer.png">

    <p>
      Later you'll be asked to review the card: that is, shown the
      question, and asked whether you know the answer or not.
    </p>

    <p>
      What makes Anki better than conventional flashcards is that it
      manages the review schedule. If you can answer a question
      correctly, the time interval between reviews gradually
      expands. So a one-day gap between reviews becomes two days, then
      six days, then a fortnight, and so on. The idea is that the
      information is becoming more firmly embedded in your memory, and
      so requires less frequent review. But if you ever miss an
      answer, the schedule resets, and you again have to build up the
      time interval between reviews.
    </p>

    <p>
      While it's obviously useful that the computer manages the
      interval between reviews, it perhaps doesn't seem like that big
      a deal.  The punchline is that this turns out to be a vastly
      more efficient way to remember information.
    </p>

    <p>
      How much more efficient?
    </p>

    <p>
      To answer that question, let's do some rough time estimates.  On
      average, it takes me about 8 seconds to review a card. Suppose I
      was using conventional flashcards, and reviewing them (say) once
      a week. If I wanted to remember something for the next 20 years,
      I'd need 20 years times 52 weeks per year times 8 seconds per
      card. That works out to a total review time of just over 2 hours
      for each card.
    </p>

    <p>
      By contrast, Anki's ever-expanding review intervals quickly rise
      past a month and then out past a year. Indeed, for my personal
      set of Anki cards the average interval between reviews is
      currently 1.2 years, and rising.  In
      an <a href="#Anki_analysis">appendix</a> below I estimate that
      for an average card, I'll only need 4 to 7 minutes of total
      review time over the entire 20 years.  Those estimates allow for
      occasional failed reviews, resetting the time interval.  That's
      a factor of more than 20 in savings over the more than 2 hours
      required with conventional flashcards!
    </p>

    <p>
      I therefore have two rules of thumb. First, if memorizing a fact
      seems worth 10 minutes of my time in the future, then I do
      it*<span class="marginnote">* I first saw an analysis along
      these lines in Gwern Branwen's review of spaced repetition:
      Gwern
      Branwen, <a href="https://www.gwern.net/Spaced-repetition">Spaced-Repetition</a>. His
      numbers are slightly more optimistic than mine &ndash; he
      arrives at a 5-minute rule of thumb, rather than 10 minutes
      &ndash; but broadly consistent. Branwen's analysis is based, in
      turn, on an analysis in: Piotr
      Wozniak, <a href="http://super-memory.com/articles/theory.htm">Theoretical
      aspects of spaced repetition in learning</a>.</span>. Second,
      and superseding the first, if a fact seems striking then into
      Anki it goes, regardless of whether it seems worth 10 minutes of
      my future time or not. The reason for the exception is that many
      of the most important things we know are things we're not sure
      are going to be important, but which our intuitions tell us
      matter. This doesn't mean we should memorize everything. But
      it's worth cultivating taste in what to memorize.
    </p>

    <p>
      The single biggest change that Anki brings about is that it
      means memory is no longer a haphazard event, to be left to
      chance. Rather, it guarantees I will remember something, with
      minimal effort. That is, <em>Anki makes memory a choice</em>.
    </p>

    <p>
      What can Anki be used for? I use Anki in all parts of my
      life. Professionally, I use it to learn from papers and books;
      to learn from talks and conferences; to help recall interesting
      things learned in conversation; and to remember key observations
      made while doing my everyday work. Personally, I use it to
      remember all kinds of facts relevant to my family and social
      life; about my city and travel; and about my hobbies. In later
      sections of the essay I describe some of the most useful
      patterns, and anti-patterns to avoid.
    </p>

    <p>
      I've used Anki to create roughly 11,000 cards over about 2 and a
      half years of regular use. That includes a 7-month break when I
      made very few new cards. When I'm keeping up with my card
      review, it takes about 15 to 20 minutes per day. If it routinely
      rises to much more than 20 minutes it usually means I'm adding
      cards too rapidly, and need to slow down. Alternately, it
      sometimes means I'm behind on my card review (which I'll discuss
      later).
    </p>

    <p>
      At a practical level, I use the desktop Anki client for entering
      new cards, and the mobile client*<span class="marginnote">* The
      desktop client is free, but the mobile client is, at the time of
      writing, 25 dollars.  Many people balk at that as &ldquo;too
      expensive&rdquo;. Personally, I've found the value is several
      orders of magnitude beyond that. Mobile Anki is certainly far
      more valuable to me than a single meal in a moderately priced
      restaurant.</span> for reviewing. I review my Anki cards while
      walking to get my morning coffee, while waiting in line, on
      transit, and so on. Provided my mind is reasonably relaxed to
      begin with, I find the review experience meditative. If it's not
      relaxed, I find review more difficult, and it can cause my mind
      to jump around.
    </p>

    <p>
      I had trouble getting started with Anki. Several acquaintances
      highly recommended it, and over the years I made multiple
      attempts to use it, each time quickly giving up. In retrospect,
      there are substantial barriers to get over if you want to make
      it a habit.
    </p>

    <p>
      What made Anki finally &ldquo;take&rdquo; for me, turning it
      into a habit, was a project I took on as a joke. I'd been
      frustrated for years at never really learning the Unix command
      line. I'd only ever learned the most basic commands. Learning
      the command line is a superpower for people who program, so it
      seemed highly desirable to know well. So, for fun, I wondered if
      it might be possible to use Anki to essentially completely
      memorize a (short!) book about the Unix command line.
    </p>

    <p>
      It was!
    </p>

    <p>
      I chose O'Reilly Media's &ldquo;Macintosh Terminal Pocket
      Guide&rdquo;, by Daniel Barrett. I don't mean I literally
      memorized the entire text of the book*<span class="marginnote">*
      I later did an experiment with Charles Dickens' &ldquo;A Tale of
      Two Cities&rdquo;, seeing if it might be possible to memorize
      the entire text. After a couple of months I concluded that it
      would work, but would not be worth the time. So I deleted all
      the cards. An interesting thing has occurred post-deletion: the
      first few sentences of the book have gradually decayed in my
      memory, and I now have no more than fragments.  I do still
      wonder what the impact would be of memorizing a good book in its
      entirety; might it influence your own language and writing, for
      instance?</span>. But I did memorize much of the conceptual
      knowledge in the book, as well as the names, syntax, and options
      for most of the commands in the book. The exceptions were things
      I had no frame of reference to imagine using. But I did memorize
      most things I could imagine using. In the end I covered perhaps
      60 to 70 percent of the book, skipping or skimming some sections
      that didn't seem relevant to me. Still, my knowledge of the
      command line increased enormously.
    </p>

    <p>
      Choosing this rather ludicrous, albeit extremely useful, goal
      gave me a great deal of confidence in Anki. It was extremely
      exciting, making it obvious that Anki would make it easy to
      learn things that would formerly have been quite difficult.
      This confidence, in turn, made it much easier to build an Anki
      habit.
    </p>

    <p>
      Doing this project also helped me learn the Anki interface and
      to experiment with different ways of posing questions. That is,
      it helped motivate me to build the skills necessary to use Anki
      well.
    </p>


    <h3>Using Anki to thoroughly read a research paper in an
    unfamiliar field</h3>

    <p>
      I find Anki a great help when reading research papers,
      particularly in fields outside my expertise. As an example of
      how this can work, I'll describe my experience reading a 2016
      paper*<span class="marginnote">* David Silver, Aja Huang, Chris
      J. Maddison, Arthur Guez <em>et
      al</em>, <a href="assets/Silver2016a.pdf">Mastering the game of
      Go with deep neural networks and tree search</a>, Nature
      (2016).</span> describing AlphaGo, the computer system from
      Google DeepMind that beat some of the world's strongest players
      of the game Go.
    </p>

    <p>
      After the match where AlphaGo beat Lee Sedol, one of the
      strongest human Go players in history, I suggested
      to <a href="https://www.quantamagazine.org/">Quanta Magazine</a>
      that I write an article for them about the
      system*<span class="marginnote">* Michael
      A. Nielsen, <a href="https://www.quantamagazine.org/is-alphago-really-such-a-big-deal-20160329/">Is
      AlphaGo Really Such a Big Deal?</a>, Quanta
      (2016).</span>. AlphaGo was a hot media topic at the time, and
      the most common angle in stories was human interest, viewing
      AlphaGo as part of a long-standing human-versus-machine
      narrative, with a few technical details filled in, mostly as
      color.</p>

    <p>
      I wanted to take a different angle in my article. Through the
      1990s and first decade of the 2000s, I believed human-or-better
      general artificial intelligence was far away. The reason was
      that over that time researchers made only slow progress building
      systems to do intuitive pattern matching, of the kind that
      underlies human cognition in areas such as human vision and
      human speech recognition, as well as in playing games such as
      Go. Despite enormous effort by AI researchers, many
      pattern-matching feats which humans find effortless remained
      impossible for machines.
    </p>

    <p>
      While we made only very slow progress on this set of problems
      for a long time, around 2011 progress began to speed up, driven
      by advances in deep neural networks. For instance, machine
      vision systems rapidly went from being hopeless to being
      comparable to human beings for certain limited tasks.  By the
      time AlphaGo was released, it was no longer correct to say we
      had no idea how to build computer systems to do intuitive
      pattern matching. While we hadn't yet nailed the problem, we
      were making rapid progress. AlphaGo was a big part of that
      story, and I wanted my article to explore this notion of
      building computer systems to capture human intuition.
    </p>

    <p>
      While excited, writing such an article was going to be
      difficult. It was going to require a deeper understanding of the
      technical details of AlphaGo than a typical journalistic
      article. Fortunately, I knew a fair amount about the general
      area of neural networks &ndash; I'd written a book about
      them*<span class="marginnote">* Michael
      A. Nielsen, <a href="http://neuralnetworksanddeeplearning.com">"Neural
      Networks and Deep Learning"</a>, Determination Press
      (2015).</span>. But I knew nothing about the game of Go, or
      about many of the ideas used by AlphaGo, based on a field known
      as reinforcement learning. I was going to need to learn this
      material from scratch, and to write a good article I was going
      to need to really understand the underlying technical material.
    </p>

    <p>
      Here's how I went about it.
    </p>

    <p>
      I began with the
      AlphaGo <a href="assets/Silver2016a.pdf">paper</a> itself. I
      began reading it quickly, almost skimming. I wasn't looking for
      a comprehensive understanding. Rather, I was doing two
      things. One, I was trying to simply identify the most important
      ideas in the paper. What were the names of the key techniques
      I'd need to learn about? Second, there was a kind of hoovering
      process, looking for basic facts that I could understand easily,
      and that would obviously benefit me. Things like basic
      terminology, the rules of Go, and so on.
    </p>

    <p>
      Here's a few examples of the kind of question I entered into
      Anki at this stage: &ldquo;What's the size of a Go
      board?&rdquo;; &ldquo;Who plays first in Go?&rdquo;; &ldquo;How
      many human training games did AlphaGo learn from?&rdquo;;
      &ldquo;Where did AlphaGo get its training data?&rdquo;;
      &ldquo;What were the names of the two main types of neural
      network AlphaGo used?&rdquo;
    </p>

    <p>
      As you can see, these are all elementary questions.  They're the
      kind of thing that are very easily picked up during an initial
      pass over the paper, with occasional digressions to search
      Google and Wikipedia, and so on. Furthermore, while these facts
      were easy to pick up in isolation, they also seemed likely to be
      useful in building a deeper understanding of other material in
      the paper.
    </p>

    <p>
      I made several rapid passes over the paper in this way, each
      time getting deeper and deeper. At this stage I wasn't trying to
      obtain anything like a complete understanding of
      AlphaGo. Rather, I was trying to build up my background
      understanding.  At all times, if something wasn't easy to
      understand, I didn't worry about it, I just keep going. But as I
      made repeat passes, the range of things that were easy to
      understand grew and grew. I found myself adding questions about
      the types of features used as inputs to AlphaGo's neural
      networks, basic facts about the structure of the networks, and
      so on.
    </p>

    <p>
      After five or six such passes over the paper, I went back and
      attempted a thorough read. This time the purpose was to
      understand AlphaGo in detail.  By now I understood much of the
      background context, and it was relatively easy to do a thorough
      read, certainly far easier than coming into the paper
      cold. Don't get me wrong: it was still challenging. But it was
      far easier than it would have been otherwise.
    </p>

    <p>
      After doing one thorough pass over the AlphaGo paper, I made a
      second thorough pass, in a similar vein. Yet more fell into
      place. By this time, I understood the AlphaGo system well. Many
      of the questions I was putting into Anki were high level,
      sometimes on the verge of original research directions. I
      certainly understood AlphaGo well enough that I was confident I
      could write the sections of my article dealing with it. (In
      practice, my article ranged over several systems, not just
      AlphaGo, and I had to learn about those as well, using a similar
      process, though I didn't go as deep.) I continued to add
      questions as I wrote my article, ending up adding perhaps
      400-500 questions in total. But by this point the hardest work
      had been done.
    </p>


    <p>
      Of course, instead of using Anki I could have taken conventional
      notes, using a similar process to build up an understanding of
      the paper. But using Anki gave me confidence I would retain much
      of the understanding over the long term.  A year or so later
      DeepMind released papers describing followup systems, known as
      AlphaGo Zero and AlphaZero*<span class="marginnote">* For
      AlphaGo Zero, see: David Silver, Julian Schrittwieser, Karen
      Simonyan, Ioannis Antonoglou <em>et
      al</em>, <a href="assets/Silver2017a.pdf">Mastering the game of
      Go without human knowledge</a>, Nature (2017). For AlphaZero,
      see: David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis
      Antonoglou <em>et
      al</em>, <a href="https://arxiv.org/abs/1712.01815">Mastering
      Chess and Shogi by Self-Play with a General Reinforcement
      Learning Algorithm</a> (2017).</span>. Despite the fact that I'd
      thought little about AlphaGo or reinforcement learning in the
      intervening time, I found I could read those followup papers
      with ease. While I didn't attempt to understand those papers as
      thoroughly as the initial AlphaGo paper, I found I could get a
      pretty good understanding of the papers in less than hour. I'd
      retained much of my earlier understanding!
    </p>

    <p>
      By contrast, had I used conventional note-taking in my original
      reading of the AlphaGo paper, my understanding would have more
      rapidly evaporated, and it would have taken longer to read the
      later papers. And so using Anki in this way gives confidence you
      will retain understanding over the long term. This confidence,
      in turn, makes the initial act of understanding more
      pleasurable, since you believe you're learning something for the
      long haul, not something you'll forget in a day or a week.
    </p>

    <span class="quote_right">OK, but what does one do with it?
    &hellip; [N]ow that I have all this power &ndash; a mechanical
    golem that will never forget and never let me forget whatever I
    chose to &ndash; what do I choose to remember?
    &ndash; <a href="https://www.gwern.net/Spaced-repetition">Gwern
    Branwen</a>
    </span>

          <p>
      This entire process took a few days of my time, spread over a
      couple of weeks. That's lot of work. However, the payoff was
      that I got a pretty good basic grounding in modern deep
      reinforcement learning.  This is an immensely important field,
      of great use in robotics, and many researchers believe it will
      play an important role in achieving general artificial
      intelligence. With a few days work I'd gone from knowing nothing
      about deep reinforcement learning to a durable understanding of
      a key paper in the field, a paper that made use of many
      techniques that were used across the entire field. Of course, I
      was still a long way from being an expert.  There were many
      important details about AlphaGo I hadn't understood, and I would
      have had to do far more work to build my own system in the area.
      But this foundational kind of understanding is a good basis on
      which to build deeper expertise.
    </p>


      <p>
	It's notable that I was reading the AlphaGo paper in support
	of a creative project of my own, namely, writing an article
	for Quanta Magazine. This is important: when possible, it's
	extremely helpful if Anki is used in service to some personal
	creative project.
      </p>

      <p>
	It's tempting instead to use Anki to stockpile knowledge
	against some future day, to think &ldquo;Oh, I should learn
	about the geography of Africa, or learn about World War II, or
	[&hellip;]&rdquo;. These are goals which, for me, are
	intellectually appealing, but which I'm not emotionally
	invested in.  I've tried this a bunch of times. It tends to
	generate cold and lifeless Anki questions, questions which I
	find hard to connect to upon later review, and where it's
	difficult to really, deeply internalize the answers. The
	problem is somehow in that initial idea I &ldquo;should&rdquo;
	learn about these things: intellectually, it seems like a good
	idea, but I've little emotional commitment.
      </p>

      <span class="quote_right">Study hard what interests you the most
      in the most undisciplined, irreverent and original manner
      possible. &ndash; Richard Feynman
      </span>
      
      <p>
	By contrast, when I'm reading in support of some creative
	project, I ask much better Anki questions.  I find it easier
	to connect to the questions and answers emotionally. I simply
	care more about them, and that makes a difference.  So while
	it's tempting to use Anki cards to study in preparation for
	some (possibly hypothetical) future use, I advise against
	it. If you can, it's better to find a way to use Anki as part
	of some creative project.
      </p>
      
	
    <h3>Using Anki to do shallow reads of papers</h3>
    
    <p>
      Most of my Anki-based reading is much shallower than my read of
      the AlphaGo paper. Rather than spending days on a paper, I'll
      typically spend 10 to 60 minutes, sometimes longer for very good
      papers. Here's a few notes on some patterns I've found useful in
      shallow reading.
    </p>

    <p>
      As mentioned above, I'm usually doing such reading as part of
      the background research for some project. I will find a new
      article (or set of articles), and typically spend a few minutes
      assessing it. Does the article seem likely to contain
      substantial insight or provocation relevant to my project
      &ndash; new questions, new ideas, new methods, new results?  If
      so, I'll have a read.
    </p>

    <p>
      This doesn't mean reading every word in the paper. Rather, I'll
      add to Anki questions about the core claims, core questions, and
      core ideas of the paper. It's particularly helpful to extract
      Anki questions from the abstract, introduction, conclusion,
      figures, and figure captions. Typically I will extract anywhere
      from 5 to 20 Anki questions from the paper. It's usually a bad
      idea to extract fewer than 5 questions &ndash; doing so tends to
      leave the paper as a kind of isolated orphan in my memory.
      Later I find it difficult to feel much connection to those
      questions. Put another way: if a paper is so uninteresting that
      it's not possible to add 5 good questions about it, it's usually
      better to add no questions at all.
    </p>

    <p>
      One failure mode of this process is if you Ankify misleading
      work, i.e., enter it into Anki. Many papers contain wrong or
      misleading statements, and if you commit such items to memory,
      you're actively making yourself stupider.
    </p>

    <p>
      How to avoid Ankifying misleading work?
    </p>

    <p>
      As an example, let me describe how I Ankified a paper I recently
      read, by the economists Benjamin Jones and Bruce
      Weinberg*<span class="marginnote">* Benjamin F. Jones and Bruce
      A. Weinberg, <a href="assets/Jones2011a.pdf">Age Dynamics in
      Scientific Creativity</a>, Proceedings of the National Academy
      of Sciences (2011).</span>. The paper studies the ages at which
      scientists make their greatest discoveries.
    </p>

      <p>
	I should say at the outset: I have no reason to think this
      paper is misleading! But it's also worth being cautious.  As an
      example of that caution, one of the questions I added to Anki
      was: &ldquo;What does Jones 2011 claim is the average age at
      which physics Nobelists made their prizewinning discovery, over
      1980-2011?&rdquo; (Answer: 48).  Another variant question was:
      &ldquo;Which paper claimed that physics Nobelists made their
      prizewinning discovery at average age 48, over the period
      1980-2011?&rdquo; (Answer: Jones 2011).  And so on.
    </p>

    <p>
      Such questions qualify the underlying claim: we now know it was
      a claim made in Jones 2011, and that we're relying on the
      quality of Jones and Weinberg's data analysis. In fact, I
      haven't examined that analysis carefully enough to regard it as
      a fact that the average age of those Nobelists is 48. But it is
      certainly a fact that their paper claimed it was 48. Those are
      different things, and the latter is the right thing to Ankify.
    </p>

    <p>
      If I'm particularly concerned about the quality of the analysis,
      I may add one or more questions about what makes such work
      difficult, e.g.: &ldquo;What's one challenge in determining the
      age of Nobel winners at the time of their discovery, as
      discussed in Jones 2011?&rdquo; Good answers include: the
      difficulty of figuring out which paper contained the
      Nobel-winning work; the fact that publication of papers is
      sometimes delayed by years; that sometimes work is spread over
      multiple papers; and so on. Thinking about such challenges
      reminds me that if Jones and Weinberg were sloppy, or simply
      made an understandable mistake, their numbers might be off.
      Now, it so happens that for this particular paper, I'm not too
      worried about such issues. And so I didn't Ankify any such
      question.  But it's worth being careful in framing questions so
      you're not misleading yourself.
    </p>
    
    <p>
      Another useful pattern while reading papers is Ankifying
      figures. For instance, here's a graph from Jones 2011 showing
      the probability a physicist made their prizewinning discovery by
      age 40 (blue line) and by age 30 (black line):
    </p>

    <center>
      <img src="assets/Jones_figure2011a.png"/>
    </center>
    
    <p>
      I have an Anki question which simply says: &ldquo;Visualize the
      graph Jones 2011 made of the probability curves for physicists
      making their prizewinning discoveries by age 30 and
      40&rdquo;. The answer is the image shown above, and I count
      myself as successful if my mental image is roughly along those
      lines. I could deepen my engagement with the graph by adding
      questions such as: &ldquo;In Jones 2011's graph of physics
      prizewinning discoveries, what is the peak probability of great
      achievement by age 40 [i.e., the highest point in the blue line
      in the graph above]?&rdquo; (Answer: about 0.8.)  Indeed, one
      could easily add dozens of interesting questions about this
      graph. I haven't done that, because of the time commitment
      associated to such questions. But I do find the broad shape of
      the graph fascinating, and it's also useful to know the graph
      exists, and where to consult it if I want more details.
    </p>

    <p>
      I said above that I typically spend 10 to 60 minutes Ankifying a
      paper, with the duration depending on my judgment of the value
      I'm getting from the paper. However, if I'm learning a
      tremendous amount, and finding it interesting, I keep reading
      and Ankifying. Really good resources are worth investing time
      in. But most papers don't fit this pattern, and you quickly
      saturate. If you feel you could easily find something more
      rewarding to read, switch over. It's worth deliberately
      practicing such switches, to avoid building a counter-productive
      habit of completionism in your reading. It's nearly always
      possible to read deeper into a paper, but that doesn't mean you
      can't easily be getting more value elsewhere.  It's a failure
      mode to spend too long reading unimportant papers.
    </p>

    <!-- 

    <p>
      The exception to this rule is when I'm new to a field. A paper
      may not be very good, but it may use many important techniques
      common to the field. Because those techniques are new to me, I
      find myself Ankifying lots of associated material.
    </p>

      <p>
	This pattern is okay, in moderation. For one thing, those
	techniques are likely to show up in multiple papers, so it
	doesn't much matter when I Ankify them. With that said, if I
	find this pattern occurring repeatedly, it usually means I
	need to refocus on truly key papers, and not get so bogged
	down in less important papers.
    </p>

      -->

    <h3>Syntopic reading using Anki</h3>
    
      <p>
	I've talked about how to use Anki to do shallow reads of
	papers, and rather deeper reads of papers. There's also a
	sense in which it's possible to use Anki not just to read
	papers, but to &ldquo;read&rdquo; the entire research
	literature of some field or subfield.  Here's how to do it.
      </p>

      <p>
	You might suppose the foundation would be a shallow read of a
	large number of papers. In fact, to really grok an unfamiliar
	field, you need to engage deeply with key papers &ndash;
	papers like the AlphaGo paper. What you get from deep
	engagement with important papers is more significant than any
	single fact or technique: you get a sense for what a powerful
	result in the field looks like. It helps you imbibe the
	healthiest norms and standards of the field. It helps you
	internalize how to ask good questions in the field, and how to
	put techniques together. You begin to understand what made
	something like AlphaGo a breakthrough &ndash; and also its
	limitations, and the sense in which it was really a natural
	evolution of the field. Such things aren't captured
	individually by any single Anki question. But they begin to be
	captured collectively by the questions one asks when engaged
	deeply enough with key papers.
      </p>

      <p>
	So, to get a picture of an entire field, I usually begin with
	a truly important paper, ideally a paper establishing a result
	that got me interested in the field in the first place. I do a
	thorough read of that paper, along the lines of what I
	described for AlphaGo. Later, I do thorough reads of other key
	papers in the field &ndash; ideally, I read the best 5-10
	papers in the field. But, interspersed, I also do shallower
	reads of a much larger number of less important (though still
	good) papers. That typically means anywhere from tens to
	hundreds of such papers.
      </p>

      <p>
	You may wonder why I don't just focus on only the most
	important papers. Part of the reason is mundane: it can be
	hard to tell what the most important papers are. Shallow reads
	of many papers can help you figure out what the key papers
	are, without spending too much time doing deeper reads of
	papers that turn out not to be so important. But there's also
	a culture that one imbibes reading the bread-and-butter papers
	of a field: a sense for what routine progress looks like, for
	the praxis of the field. That's valuable too, especially for
	building up an overall picture of where the field is at, and
	to stimulate questions on my own part. Indeed, while I don't
	recommend spending a large fraction of your time reading bad
	papers, it's certainly possible to have a good conversation
	with a bad paper. Stimulus is found an unexpected places.
      </p>

      <p>
	Over time, this is a form of what Mortimer Adler and Charles
	van Doren dubbed <em>syntopic
	reading</em>*<span class="marginnote">* In their marvelous
	&ldquo;How to Read a Book&rdquo;: Mortimer J. Adler and
	Charles van Doren, &ldquo;How to Read a Book: The Classic
	Guide to Intelligent Reading&rdquo; (1972)</span>. I build up
	an understanding of an entire literature: what's been done,
	what's not yet been done. I start to identify open problems,
	questions that I'd personally like answered, but which don't
	yet seem to have been answered. I identify tricks,
	observations that seem pregnant with possibility, but whose
	import I don't yet know. And, sometimes, I identify what seem
	to me to be field-wide blind spots. I add questions about all
	these to Anki as well. In this way, Anki is a medium
	supporting my creative research. It has some shortcomings as
	such a medium, since it's not designed with supporting
	creative work in mind &ndash; it's not, for instance, equipped
	for lengthy, free-form exploration inside a scratch space. But
	even without being designed in such a way, it's helpful as a
	creative support.
      </p>
      
    <p>
      I've been describing how I use Anki to learn subjects which are
      largely new to me.  By contrast, with a subject I already know
      well, my curiosity and my model of the subject are often already
      so strong that it's easy to integrate new facts. I still find
      Anki useful, but it's definitely most useful in new areas.  The
      great English mathematician John Edensor Littlewood
      wrote*<span class="marginnote">* In &ldquo;Littlewood's
      miscellany&rdquo;, edited by Béla Bollobás (1986).</span>:
    </p>

    <blockquote>
      I have tried to learn mathematics outside my fields of interest;
      after any interval I had to begin all over again.
    </blockquote>

    <p>
      This captures something of the immense emotional effort I used
      to find required to learn new areas. Without a lot of drive, it
      was extremely difficult to make a lot of material in a new field
      stick. Anki does much to solve that problem. In a sense, it's an
      emotional prosthetic, actually helping create the drive I need
      to achieve understanding. It doesn't do the entire job &ndash;
      as mentioned earlier, it's very helpful to have other
      commitments (like a creative project, or people depending on me)
      to help create that drive.  Nonetheless, Anki helps give me
      confidence that I can simply <em>decide</em> I'm going to read
      deeply into a new area, and retain and make sense of what I
      learn. This has worked for all areas of conceptual understanding
      where I've tried it*<span class="marginnote">* There are many
      types of motor skills and problem-solving skills I haven't tried
      using Anki for, and I'm not sure if it could be adapted.</span>.
    </p>

    <p>
      One surprising consequence of reading in this way is how much
      more enjoyable it becomes. I've always enjoyed reading, but
      starting out in a challenging new area was sometimes a real
      slog, and I was often bedeviled by doubts that I would ever
      really get into the area. That doubt, in turn, made it less
      likely that I would succeed. Now I have confidence that I can go
      into a new field and quickly attain a good, relatively deep
      understanding, an understanding that will be durable. That
      confidence makes reading even more pleasurable.
    </p>


    <h3>More patterns of Anki use</h3>


    <p>
      Having looked at the use of Anki for reading technical papers,
      let's return to general patterns of use.
    </p>

      <p>
	<strong>Make most Anki questions and answers as atomic as
	  possible:</strong> That is, both the question and answer
	  express just one idea. As an example, when I was learning
	  the Unix command line, I entered the question: &ldquo;How to
	  create a soft link from <code>linkname</code>
	  to <code>filename</code>?&rdquo; The answer was:
	  &ldquo;<code>ln -s filename
	  linkname</code>&rdquo;. Unfortunately, I routinely got this
	  question wrong.
    </p>

    <p>
      The solution was to break the question into two pieces. One
      piece was: &ldquo;What's the basic command and option to create
      a Unix soft link?&rdquo; Answer: &ldquo;<code>ln -s
      &hellip;</code>&rdquo;. And the second piece was: &ldquo;When
      creating a Unix soft link, in what order do <code>linkname</code> and
      <code>filename</code> go?&rdquo; Answer: &ldquo;<code>filename
	linkname</code>&rdquo;.
    </p>

    <p>
      Breaking this question into more atomic pieces turned a question
      I routinely got wrong into two questions I routinely got
      right*<span class="marginnote">* An even more atomic version
      would be to break the first question into &ldquo;What's the Unix
      command to create a link?&rdquo; and &ldquo;What's the option to
      the <code>ln</code> command to create a soft link?&rdquo; In
      practice, I've known for years that <code>ln</code> is the
      command to create a link, and so this wasn't
      necessary.</span>. Most of all: when I wanted to create a Unix
      soft link in practice, I knew how to do it.
    </p>

    <p>
      I'm not sure what's responsible for this effect. I suspect it's
      partly about focus.  When I made mistakes with the combined
      question, I was often a little fuzzy about where exactly my
      mistake was. That meant I didn't focus sharply enough on the
      mistake, and so didn't learn as much from my failure. When I
      fail with the atomic questions my mind knows exactly where to
      focus.
    </p>

    <p>
      In general, I find that you often get substantial benefit from
      breaking Anki questions down to be more atomic. It's a powerful
      pattern for question refactoring.
    </p>

    <p>
      Note that this doesn't mean you shouldn't also retain some
      version of the original question. I still want to know how to
      create a soft link in Unix, and so it's worth keeping the
      original question in Anki. But it becomes an integrative
      question, part of a hierarchy of questions building up from
      simple atomic facts to more complex ideas.
    </p>

    <p>
      Incidentally, just because a question is atomic doesn't mean it
      can't involve quite complex, high-level concepts. Consider the
      following question, from the field of general relativity:
      &ldquo;What is the <em>dr<sup>2</sup></em> term in the
      Robertson-Walker metric?&rdquo; Answer:
      <em>dr<sup>2</sup>/(1-kr^2)</em>. Now, unless you've studied
      general relativity that question probably seems quite
      opaque. It's a sophisticated, integrative question, assuming you
      know what the Robertson-Walker metric is,
      what <em>dr<sup>2</sup></em> means, what
      <em>k</em> means, and so on. But conditional on that background
      knowledge, it's quite an atomic question and answer.
    </p>

    <p>
      One benefit of using Anki in this way is that you begin to
      habitually break things down into atomic questions. This sharply
      crystallizes the distinct things you've learned. Personally, I
      find that crystallization satisfying, for reasons I (ironically)
      find difficult to articulate. But one real benefit is that later
      I often find those atomic ideas can be put together in ways I
      didn't initially anticipate. And that's well worth the trouble.
    </p>

      <p>
	<strong>Anki use is best thought of as a virtuoso skill, to be
	  developed:</strong> Anki is an extremely simple program: it
	  lets you enter text or other media, and then shows you that
	  media on a schedule determined by your responses. Despite
	  that simplicity, it's an incredibly powerful tool. And, like
	  many tools, it requires skill to use well, skill that can be
	  developed over time. It's worth thinking of Anki as a skill
	  that can be taken to virtuoso levels, and attempting to
	  continue to level up on the way to such virtuosity.
      </p>
      
      <p>
	<strong>Anki isn't just a tool for memorizing simple facts.
	  It's a tool for understanding almost anything.</strong> It's
	  a common misconception that Anki is just for memorizing
	  simple raw facts, things like vocabulary items and basic
	  definitions. But as we've seen, it's possible to use Anki
	  for much more advanced types of understanding. My questions
	  about AlphaGo began with simple questions such as &ldquo;How
	  large is a Go board?&rdquo;, and ended with high-level
	  conceptual questions about the design of the AlphaGo systems
	  &ndash; many questions on subjects such as how AlphaGo
	  avoided over-generalizing from training data, the
	  limitations of convolutional neural networks, and so on.
      </p>

      <p>
	Part of developing Anki as a virtuoso skill is developing the
	ability to use it for types of understanding beyond basic
	facts. Indeed, many of the observations I've made (and will
	make, below) about how to use Anki are really about what it
	means to understand something. Break things up into atomic
	facts. Build rich hierarchies of interconnections and
	integrative questions. Don't put in orphan questions. Patterns
	for how to engage with reading material, patterns (and
	anti-patterns) for questions types, patterns for the kinds of
	things you'd like to memorize. Anki skills concretely
	instantiate your theory of how you understand; developing
	those skills will help you understand better. It's too strong
	to say that to be a virtuoso Anki user is to be a virtuoso in
	understanding. But there's some truth to it.
      </p>

      <p>
      <strong>Use one big deck:</strong> Anki allows you to organize
      cards into decks and subdecks. Some people use this to create a
      complicated organizational structure. I used to do this, but
      I've gradually*<span class="marginnote">* It's gradual because
      questions sometimes need to be rewritten due to the changed
      context. For instance, both my Emacs and Unix command line decks
      had very similar questions, along the lines of: &ldquo;How to
      delete a word?&rdquo; Those questions need to be rewritten,
      e.g. as: &ldquo;In Emacs, how to delete a word?&rdquo; (This, by
      the way, may seem a strange question for a long-time Emacs user
      such as myself. In fact, I've used Anki to help me change the
      way I delete words in Emacs, which is why I have an Anki
      question on the subject. I have made a bunch of improvements to
      my Emacs workflow this way.)</span>  merged my decks and
      subdecks into one big deck. The world isn't divided up into
      neatly separated components, and I believe it's good to collide
      very different types of questions. One moment Anki is asking me
      a question about the temperature chicken should be cooked
      to. The next: a question about the JavaScript API. Is this
      mixing doing me any real good? I'm not sure. I have not, as yet,
      found any reason to use JavaScript to control the cooking of a
      chicken. But I don't think this mixing does any harm, and hope
      it is creatively stimulating, and helps me apply my knowledge in
      unusual contexts.
    </p>

    <p>
      <strong>Avoid orphan questions:</strong> Suppose I'm reading
      online and stumble across a great article about the grooming
      habits of the Albanian giant mongoose, a subject I never
      previously knew I was interested in, but which turns out to be
      fascinating. Pretty soon I've Ankified 5 to 10 questions. That's
      great, but my experience suggests that in a few months I'll
      likely find those questions rather stale and disconnected, and
      frequently get them wrong. I believe the reason is that those
      questions are too far from my other interests, and I will have
      lost the context that made me interested.
    </p>

    <p>
      I call these <em>orphan questions</em>, because they're not
      closely related to anything else in my memory. It's not bad to
      have a few orphan questions in Anki &ndash; it can be difficult
      to know what will turn out to be of only passing interest, and
      what will grow into a substantial interest, connected to my
      other interests. But if a substantial minority of your questions
      are orphans, that's a sign you should concentrate more on
      Ankifying questions related to your main creative projects, and
      cut down on Ankifying tangential material.
    </p>

    <p>
      It's particularly worth avoiding lonely orphans: single
      questions that are largely disconnected from everything
      else. Suppose, for instance, I'm reading an article on a new
      subject, and I learn an idea that seems particularly useful. I
      make it a rule to never put in one question. Rather, I try to
      put at least two questions in, preferably three or more. That's
      usually enough that it's at least the nucleus of a bit of useful
      knowledge. If it's a lonely orphan, inevitably I get the
      question wrong all the time, and it's a waste to have entered it
      at all.
    </p>
 
    <p>
      <strong>Don't share decks:</strong> I'm often asked whether I'd
      be willing to share my Anki decks. I'm not. Very early on I
      realized it would be very useful to put personal information in
      Anki. I don't mean anything terribly personal &ndash; I'd never
      put deep, dark secrets in there. Nor do I put anything requiring
      security, like passwords. But I do put some things I wouldn't
      sling about casually.
    </p>

    <p>
      As an example, I've a (very short!) list of superficially
      charming and impressive colleagues who I would never work with,
      because I've consistently seen them treat other people
      badly. It's helpful to Ankify some details of that treatment, so
      I can clearly remember why that person should be avoided. This
      isn't the kind of information that is right to spread casually:
      I may have misinterpreted the other person's actions, or have
      misunderstood the context they were operating in. But it's
      personally useful for me to have in Anki.
    </p>

    <p>
      <strong>Construct your own decks:</strong> The Anki site
      has <a href="https://ankiweb.net/shared/decks/">many shared
      decks</a>, but I've found only a little use for them. The most
      important reason is that making Anki cards is an act of
      understanding in itself.  That is, figuring out good questions
      to ask, and good answers, is part of what it means to understand
      a new subject well. To use someone else's cards is to forgo much
      of that understanding.
    </p>

    <p>
      I believe the act of constructing the cards actually helps with
      memory. Memory researchers have repeatedly found that the more
      elaborately you encode a memory, the stronger the memory will
      be. By elaborative encoding, they mean essentially the richness
      of the associations you form.
    </p>

    <p>
      For instance, it's possible to try to remember as an isolated
      fact that 1962 was the year the first telecommunications
      satellite, Telstar, was put into orbit. But a better way of
      remembering it is to relate that fact to others. For instance, I
      personally find it fascinating that Telstar was put into orbit
      the year <em>before</em> the introduction of ASCII, arguably the
      first modern digital standard for communicating text. Humanity
      had a telecommunications satellite before we had a digital
      standard for communicating text! Finding that kind of connection
      is an example of an elaborative encoding.
    </p>

    <p>
      The act of constructing an Anki card is itself nearly always a
      form of elaborative encoding. It forces you to think through
      alternate forms of the question, to consider the best possible
      answers, and so on. I believe this is true for even the most
      elementary cards. And it certainly becomes true if you construct
      more complex cards, cards relating the basic fact to be
      remembered to other ideas (like the Telstar-ASCII link),
      gradually building up a web of richly interrelated ideas.
    </p>

    <p>
      With that said, there are some valuable deck-sharing
      practices. For instance, there are communities of medical
      students who find value in sharing and sometimes collaboratively
      constructing decks*<span class="marginnote">* See
      the <a href="https://www.reddit.com/r/medicalschoolanki/">MedicalSchoolAnki
      subreddit</a>, which contains frequent discussion of the best
      decks, how to use them, as well as an ever-changing canon of
      best decks to use for different purposes. See also the paper:
      Michael Hart-Matyas <em>et
      al</em>, <a href="assets/Hart-Matyas2018.pdf">Twelve tips for
      medical students to establish a collaborative flashcard
      project</a>, Medical Teacher (2018).</span>. If nothing else, by
      using a high-quality deck constructed by someone else you may
      learn good new patterns for questions, patterns that will help
      you construct better questions in future. I've also found value
      in shared decks containing very elementary questions, such
      as <a href="https://ankiweb.net/shared/info/685421036">art
      decks</a> which ask questions such as who painted a particular
      painting. But for deeper kinds of understanding, I've not yet
      found good ways of using shared decks.
    </p>

    <p>
      <strong>95% of the value of Anki comes from 5% of the
      features:</strong> I don't use most of Anki's features. Anki has
      ways of auto-generating cards, of tagging cards, a plugin
      ecosystem, and much else. In practice, I rarely use any of these
      features. My cards are always one of two types: the majority are
      simple question and answer; a substantial minority are what's
      called a <em>cloze</em>: a kind of fill-in-the-blanks test. For
      instance, I'll use clozes to test myself on favorite quotes:
    </p>

    <blockquote>
      &ldquo;if the personal computer is truly a __ then the use of it
      would actually change the __ of an __", __, __&rdquo; (Answer:
      new medium, thought patterns, entire civilization, Alan Kay,
      1989).
    </blockquote>

    <p>
      Clozes can also be used to pose questions not involving quotes:
    </p>
    
    <blockquote>
      The Adelson illusion is also known as the ___ illusion. (Answer:
      checker-shadow)
    </blockquote>

    <p>
      Why not use more of Anki's features? Part of the reason is that
      I get an enormous benefit from just the core features.
      Furthermore, learning to use this tiny set of features well has
      required a lot of work. A basketball and hoop are simple pieces
      of equipment, but you can spend a lifetime learning to use them
      well. Similarly, basic Anki practice can be developed
      enormously. And so I've concentrated on learning to use those
      basic features well.
    </p>

    <p>
      I know many people who try Anki out, and then go down a rabbit
      hole of figuring out how to learn as many features as possible
      so they can use it &ldquo;efficiently&rdquo;. Usually, they're
      chasing 1% improvements. Often, those people ultimately give up
      Anki as &ldquo;too difficult&rdquo;, which is often a synonym
      for &ldquo;I got nervous I wasn't using it
      perfectly&rdquo;. This is a pity. As I mentioned above, Anki
      offers something like a 20-fold improvement over (say) ordinary
      flashcards. And so they're giving up a 2,000% improvement
      because they were worried they were missing a few final 5%, 1%
      and (in many cases) 0.1% improvements. This kind of rabbit hole
      seems to be especially attractive to programmers.
    </p>

    <p>
      For this reason, when someone is getting started I advise not
      using any advanced features, and not installing any
      plugins. Don't, in short, come down with a bad case of
      programmer's efficiency disease. Learn how to use Anki for basic
      question and answer, and concentrate on exploring new patterns
      within that paradigm. That'll serve you far better than any
      number of hours spent fiddling around with the features. Then,
      if you build a regular habit of high-quality Anki use, you can
      start experimenting with more advanced features.
    </p>

    <p>
      <strong>The challenges of using Anki to store facts about
      friends and family:</strong> I've experimented with using Anki
      to store (non-sensitive!) questions about friends and family. It
      works well for things like &ldquo;Is [my friend] a vegan?&rdquo;
      But my use has run somewhat aground on thornier questions. For
      instance, suppose I talk with a new friend about their kids, but
      have never met those kids. I might put in questions like
      &ldquo;What is the name of [my friend's] eldest child?&rdquo;
      Or, if we'd chatted about music, I might put in: &ldquo;What is
      a musician [my friend] likes?&rdquo;
    </p>

    <p>
      This kind of experiment is well intentioned. But posing such
      questions often leaves me feeling uncomfortable. It seems too
      much like faking interest in my friends. There's a pretty strong
      social norm that if you remember your friends' taste in music or
      their kids' names, it's because you're interested in that
      friend. Using a memory aid feels somehow ungenuine.
    </p>

    <p>
      I've talked with several friends about this. Most have told me
      the same thing: they appreciate me going to so much trouble in
      the first place, and find it charming that I'd worry so much
      about whether it was ungenuine. So perhaps it's a mistake to
      worry.  Nonetheless, I still have trouble with it. I have
      adopted Anki for less personal stuff &ndash; things like
      people's food preferences. And maybe over time I'll use it for
      storing more personal facts. But for now I'm taking it slow.
    </p>

    <p>
      <strong>Procedural versus declarative memory:</strong> There's a
      big difference between remembering a fact and mastering a
      process.  For instance, while you might remember a Unix command
      when cued by an Anki question, that doesn't mean you'll
      recognize an opportunity to use the command in the context of
      the command line, and be comfortable typing it out. And it's
      still another thing to find novel, creative ways of combining
      the commands you know, in order to solve challenging problems.
    </p>

    <p>
      Put another way: to really internalize a process, it's not
      enough just to review Anki cards. You need to carry out the
      process, in context. And you need to solve real problems with
      it.
    </p>

    <p>
      With that said, I've found the transfer process relatively
      easy. In the case of the command line, I use it often enough
      that I have plenty of opportunities to make real use of my
      Ankified knowledge of the command line. Over time, that
      declarative knowledge is becoming knowledge I routinely use in
      context. That said, it'd be good to better understand when the
      transfer works and when it doesn't. Even better would be a
      memory system that integrates into my actual working
      environment. For instance, it could query me on Unix commands,
      while placing me at an actual command line. Or perhaps it would
      ask me to solve higher-level problems, while at the command
      line.
    </p>

    <p>
      I've tried one experiment in this vein: miming the action of
      typing commands while I review my Anki cards. But my subjective
      impression was that it doesn't work so well, and it was also
      quite annoying to do. So I stopped.
    </p>

    <p>
      <strong>Getting past &ldquo;names don't matter&rdquo;:</strong>
      I'm a theoretical physicist by training. There is a famous story
      in physics, told by Richard Feynman, dismissing the value of
      knowing names. As a child, he was out playing in a field with a
      know-it-all kid. Here's what happened, in Feynman's
      telling*<span class="marginnote">* Richard P. Feynman,
      &ldquo;What Do You Care What Other People Think? Further
      Adventures of a Curious Character&rdquo; (1989).</span>:
    </p>

    <blockquote>
	One kid says to me, &ldquo;See that bird?  What kind of bird is
	that?&rdquo;

	<br><br>I said, &ldquo;I haven't the slightest idea what kind
	of a bird it is.&rdquo;

	<br><br>He says, &ldquo;It'a brown-throated thrush. Your
	father doesn't teach you anything!&rdquo;

	<br><br>But it was the opposite. He [Feynman's father] had
	already taught me: &ldquo;See that bird?&rdquo; he
	says. &ldquo;It's a Spencer's warbler.&rdquo; (I knew he
	didn't know the real name.)  &ldquo;Well, in Italian, it's
	a <em>Chutto Lapittida</em>. In Portuguese, it's a <em>Bom da
	Peida</em>&hellip; You can know the name of that bird in all
	the languages of the world, but when you're finished, you'll
	know absolutely nothing whatever about the bird! You'll only
	know about humans in different places, and what they call the
	bird. So let's look at the bird and see what
	it's <em>doing</em> &mdash; that's what counts.&rdquo; (I
	learned very early the difference between knowing the name of
	something and knowing something.)
    </blockquote>

    <p>
      Feynman (or his father) goes on to a thoughtful discussion of
      real knowledge: observing behavior, understanding the reasons
      for it, and so on.
    </p>

    <p>
      It's a good story. But it goes too far: names do matter. Maybe
      not as much as the know-it-all kid thought, and they're not
      usually a deep kind of knowledge. But they're the foundation
      that allows you to build up a network of knowledge.
    </p>

      <p>
	This trope that names don't matter was repeatedly drilled into
	me during my scientific training. When I began using Anki, at
	first I felt somewhat silly putting questions about names for
	things into the system. But now I do it enthusiastically,
	knowing that it's an early step along the way to
	understanding.
      </p>

    <p>
      Anki is useful for names of all kinds of things, but I find it
      particularly helpful for non-verbal things. For instance, I put
      in questions about artworks, like: &ldquo;What does the artist
      <a href="https://www.emilyhare.co.uk/">Emily Hare's</a> painting
      <em>Howl</em> look like?&rdquo; Answer:
    </p>

    <center>
    <img src="assets/EmilyHareHowl.png">
    </center>

    <p>
      I put that question in for two reasons. The main reason is that
      I like to remember the experience of the painting from time to
      time. And the other is to put a name to the painting. If I
      wanted to think more analytically about the painting &ndash;
      say, about the clever use of color gradients &ndash; I could add
      more detailed questions. But I'm actually pretty happy just
      committing the experience of the image to memory.
    </p>

    <p>
      <strong>What do you do when you get behind?</strong> Anki
      becomes challenging when you get behind with cards. If you skip
      a day or two &ndash; or fifty &ndash; the cards begin to back
      up. It's intimidating to come back to find you have 500 cards to
      review in a day. Even worse, if you fall out of the Anki habit,
      you can get a very long way behind. I largely stopped using Anki
      for a 7-month period, and came back to thousands of backlogged
      cards.
    </p>

    <p>
      Fortunately, it wasn't that hard to catch up. I set myself
      gradually increasing quotas (100, 150, 200, 250, and eventually
      300) of cards per day, and worked through those quotas each day
      for several weeks until I'd caught up.
    </p>

    <p>
      While this wasn't too difficult, it was somewhat demoralizing
      and discouraging. It'd be better if Anki had a &ldquo;catch
      up&rdquo; feature that would spread the excess cards over the
      next few weeks in your schedule. But it doesn't. In any case,
      this is a gotcha, but it's possible to address.
    </p>

      <p>
	<strong>Using Anki for APIs, books, videos, seminars,
	  conversations, the web, events, and places:</strong> Nearly
	  everything I've said about Ankifying papers applies also to
	  other resources. Here's a few tips. I've separated out the
	  discussion for APIs into an appendix, which you can
	  <a href="#Anki_APIs">read below</a>, if interested.
      </p>

      <p>
	For seminars and conversations with colleagues I find it
	surprisingly helpful to set Anki quotas. For instance, for
	seminars I try to find at least three high-quality questions
	to Ankify. For extended conversations, at least one
	high-quality question to Ankify. I've found that setting
	quotas helps me pay more attention, especially during
	seminars. (I find it much easier <em>a priori</em> to pay
	attention in one-on-one conversation.)
      </p>

      <p>
	I'm more haphazard about videos, events, and places. It'd be
	good to, say, systematically Ankify 3-5 questions after going
	on an outing or to a new restaurant, to help me remember the
	experience. I do this sometimes. But I haven't been that
	systematic.
      </p>

      <p>
	I tend to Ankify in real time as I read papers and books. For
	seminars, conversations, and so on I prefer to immerse myself
	in the experience. Instead of getting out Anki, I will quickly
	make a mental (or paper) note of what I want to Ankify. I then
	enter it into Anki later. This requires some discipline; it's
	one reason I prefer to set a small quota, so that I merely
	have to enter a few questions later, rather than dozens.
      </p>
	
      <p>
	One caution is with books: reading an entire book is a
	substantial commitment, and adding Anki questions regularly
	can slow you down a lot. It's worth keeping this in mind when
	deciding how much to Ankify. Sometimes a book is so dense with
	great material that it's worth taking the time to add lots of
	questions. But unmindfully Ankifying everything in sight is a
	bad habit, one I've occasionally fallen into.
      </p>

      <p>
	What you Ankify is not a trivial choice: Ankify things that
	serve your long-term goals. In some measure we become what we
	remember, so we must be careful what we
	remember*<span class="marginnote">* With apologies to Kurt
	Vonnegut, who said: &ldquo;We are what we pretend to be, so we
	must be careful about what we pretend to
	be.&rdquo;.</span>. This is always true, but Anki makes it
	especially true.
      </p>

    <p>
      With all that said, one fun pattern is to go back to my old,
      pre-Anki notes on books, and to Ankify them. This can often be
      done quickly, and gives me a greater return on the time I've
      invested in now mostly-forgotten books*<span class="marginnote"
      style="top: -20px;">* Friends sometimes complain that many books
      are over-padded essays.  Perhaps a benefit of such padding is
      that it enforces an Anki-like spaced repetition, since readers
      take weeks to read the book.  This may be an inefficient way to
      memorize the main points, but is better than having no memory of
      the book at all.</span>.
    </p>

    <p>
      Something I haven't yet figured out is how to integrate Anki
      with note taking for my creative projects. I can't replace
      note-taking with Anki &ndash; it's too slow, and for many things
      a poor use of my long-term memory. On the other hand, there are
      many benefits to using Anki for important items &ndash; fluid
      access to memory is at the foundation of so much creative
      thought.<span class="quote_right">Speed of associative thought
      is, I believe, important in creative work. &ndash; John
      Littlewood</span> In practice, I find myself instinctively and
      unsystematically doing some things as notes, others as Anki
      questions, and still other things as both. Overall, it works
      okay, but my sense is that it could be a lot better if I applied
      more systematic thought and experimentation. Part of the problem
      is that I don't have a very good system for note-taking, period!
      If I worked more on that, I suspect the whole thing would get a
      lot better. Still, it works okay.
    </p>

      <p>
	<strong>Avoid the yes/no pattern:</strong> One bad habit I
	sometimes slide into is having lots of questions with yes/no
	answers. For instance, here's a (bad) question I added when
	learning about graphical models in machine learning:
      </p>

      <blockquote>
	Is computing the partition function intractable for most
	graphical models?
      </blockquote>

      <p>
	The answer is &ldquo;yes&rdquo;. That's fine, as far as it
	goes. But it'd help my understanding to elaborate the ideas in
	the question. Can I add a question about for which graphical
	models the partition function is tractable? Can I give an
	example of a graphical model for which the partition function
	is certainly intractable? What does it mean for computing the
	partition function to be intractable anyway?
      </p>



    <h2>Part II: Principles of Personal Memory Systems</h2>

      <p>
	In the first part of this essay we looked at a particular
	personal memory system, Anki, through the lens of my personal
	experience. In the second part of this essay we'll consider
	the broader principles underlying personal memory
	systems. We'll discuss how important memory is (or is not) as
	a cognitive skill.  We'll review the role of cognitive science
	in building personal memory systems, and speculate a little on
	the development of novel personal memory systems. And we'll
	finish with the question: who should be working on tools to
	augment human cognition?
      </p>
      
      <h3>How important is long-term memory, anyway?</h3>
      
      <p>
	Long-term memory is sometimes disparaged. It's common for
	people to denigrate &ldquo;rote memory&rdquo;, especially in
	the classroom. And I've heard from many people that they
	dropped some class &ndash; organic chemistry is common &ndash;
	because it was &ldquo;just a bunch of facts, and I wanted
	something involving more understanding&rdquo;.
      </p>

      
      <p>
	I won't ddefend bad classroom teaching, or the way organic
	chemistry is often taught.  But it's a mistake to
	underestimate the importance of memory. I used to believe such
	tropes about the low importance of memory. But I now believe
	memory is at the foundation of our cognition.
      </p>

      <p>
	There are two main reasons for this change, one a personal
	experience, the other based on evidence from cognitive
	science.
      </p>

      <p>
	Let me begin with the personal experience.
      </p>

      <p>
	Over the years, I've often taught technical subjects such as
	quantum mechanics. Quantum mechanics has a reputation as a
	difficult subject, and people do, indeed, often get stuck.
	When I talk with people about the difficulties they're having,
	they often mention high-level conceptual problems. But when
	you dig down it usually turns out they're having a hard time
	with basic notation and basic terminology. Every sentence is a
	struggle. And when you clear up those elementary things, their
	high-level confusion often immediately vanishes.
      </p>

      <p>
	This led me to suspect that the high-level problems people
	think they're having trouble with often aren't the real
	problem. Rather, it's remembering the basics.
      </p>

      <p>
	This was no more than a suspicion for years. But that changed
	when I began using Anki to understand subjects such as AlphaGo
	and deep reinforcement learning. I found it almost unsettling
	how much easier Anki made learning such subjects.  I thought
	my trouble in learning new technical subjects was
	sophisticated, high-level problems (just like my quantum
	mechanics students). But it turns out I was making exactly the
	same mistake as my quantum mechanics students! The root of my
	difficulties wasn't high level at all. Rather, I just needed
	to better master the basics.
      </p>

      <p>
	Imagine you're a student of French, and someone asks you to
	compose a sonnet in French. You're having trouble finding a
	good theme, striking sentiments and images, and so on. Your
	troubles seem complex. But your real problem is that you're
	only confident with 200 words of French vocabulary. Without
	mastery of the basics, complex work is nearly impossible. And
	mastering the basics is often mostly a matter of memory.
      </p>

      <p>
	This experience of how much easier Anki made learning a new
	technical subject greatly increased my visceral appreciation
	for the importance of memory.
      </p>

      <p>
	There are also many results from cognitive science on the key
	role memory plays in cognition.
      </p>

      <p>
	One striking line is the work done by the researchers Adriaan
	de Groot and Herbert Simon, studying how people acquire
	expertise, focusing particularly on
	chess*<span class="marginnote">* See, for instance, Herbert A.
	Simon, <a href="assets/Simon1974.pdf">How Big is a Chunk?</a>,
	Science (1974), and Adriaan de Groot, <em>Thought and Choice
	in Chess</em>, Amsterdam University Press (2008, reprinted
	from 1965).</span>. They found that world-class chess experts
	saw the board differently to beginners. A beginner would see
	&ldquo;a pawn here, a rook there&rdquo;, and so on, a series
	of individual pieces. Masters, by contrast, saw much more
	elaborate &ldquo;chunks&rdquo;: combinations of pieces that
	they recognized as a unit, and were able to reason about at a
	higher level of abstraction than the individual pieces.
      </p>

      <p>
	Simon estimated chess masters learn between 25,000 and 100,000
	of these chunks during their training, and that learning the
	chunks was a key element in becoming a first-rate chess
	player. Such players really see chess positions very
	differently from beginners.
      </p>

      <p>
	Why does learning to recognize and reason about such chunks
	help so much in developing expertise? Here's a speculative,
	informal model &ndash; as far as I know, it hasn't been
	validated by cognitive scientists, so don't take it too
	seriously. I'll describe it in the context of mathematics,
	instead of chess, since mathematics is an area where I have
	experience talking with people at all ranges of ability, from
	beginners to accomplished professional mathematicians.
      </p>

      <p>
	Many people's model of accomplished mathematicians is that
	they are astoundingly bright, with very high IQs, and the
	ability to deal with very complex ideas in their mind. A
	common perception is that their smartness gives them the
	ability to deal with very complex ideas. Basically, they have
	a higher horsepower engine.
      </p>

      <p>
	It's true that top mathematicians are usually very bright. But
	here's a different explanation of what's going on. It's that,
	per Simon, many top mathematicians have, through hard work,
	internalized many more complex mathematical chunks than
	ordinary humans. And what this means is that mathematical
	situations which seem very complex to the rest of us seem very
	simple to them. So it's not that they have a higher horsepower
	mind, in the sense of being able to deal with more
	complexity. Rather, their prior learning has given them better
	chunking abilities, and so situations most people would see as
	complex they see as simple, and they find it much easier to
	reason about.
      </p>

      <p>
	Now, the concept of chunks used by Simon in his study of chess
	players actually came from a famous 1956 paper by George
	Miller, &ldquo;The Magical Number Seven, Plus or Minus
	Two&rdquo;*<span class="marginnote">* George
	A. Miller, <a href="http://psychclassics.yorku.ca/Miller/">The
	Magical Number Seven, Plus or Minus Two: Some Limits on our
	Capacity for Processing Information</a> (1956).</span>.
	Miller argued that the capacity of working memory is roughly
	seven chunks. In fact, it turns out that there is variation in
	that number from person to person, and a substantial
	correlation between the capacity of an individual's working
	memory and their general intellectual ability
	(IQ)*<span class="marginnote">* A review of the correlation
	may be found in Phillip L. Ackerman, Margaret E. Beier, and
	Mary O. Boyle, <a href="assets/Ackerman2006.pdf">Working
	Memory and Intelligence: The Same or Different Constructs?</a>
	Psychological Bulletin (2006).</span>. The better your working
	memory, the higher your IQ, and vice versa.
      </p>

      <p>
	Exactly what Miller meant by chunks he left somewhat vague,
	writing:
      </p>

      <blockquote>
	The contrast of the terms bit and chunk also serves to
	highlight the fact that we are not very definite about what
	constitutes a chunk of information. For example, the memory
	span of five words that Hayes obtained&hellip; might just as
	appropriately have been called a memory span of 15 phonemes,
	since each word had about three phonemes in it. Intuitively,
	it is clear that the subjects were recalling five words, not
	15 phonemes, but the logical distinction is not immediately
	apparent. We are dealing here with a process of organizing or
	grouping the input into familiar units or chunks, and a great
	deal of learning has gone into the formation of these familiar
	units.
      </blockquote>

      <p>
	Put another way, in Miller's account the chunk was effectively
	the <em>basic unit</em> of working memory. And so Simon and
	his collaborators were studying the basic units used in the
	working memory of chess players. If those chunks are more
	complex, then your working memory has a higher effective
	capacity. In partciular, someone with a lower IQ but able to
	call on more complex chunks may be to reason about more
	complex situations than someone with a higher IQ but less
	complex internalized chunks.
      </p>

      <p>
	In other words, having more chunks memorized in some domain is
	little bit like an effective boost to your IQ in that
	domain*<span class="marginnote">* The chunks seem to only be
	partially conscious on the part of the experts. It's also
	unclear exactly how they arise. It seems likely to me that
	it's in part due to straight up memory, and in considerable
	part an outcome of analysis and problem-solving. Certainly,
	effective use of personal memory systems should be coupled
	to analysis and problem-solving.  </span>.
      </p>

      <p>
	Okay, that's a speculative informal model. Regardless of
	whether it's correct, there is much evidence that long-term
	memory is at the heart of effective cognition, including our
	ability to understand, to problem solve, and to create.
	Personal memory systems such as Anki make all these other
	aspects of your cognition more effective. It's as though
	you've somehow obtained a magic memory. And so such personal
	memory systems are key tools for augmenting human cognition.
      </p>

    <h3>Distributed practice</h3>

    <p>
      Why does Anki work?  In this section we briefly look at one of
      the key underlying ideas from cognitive science, known
      as <em>distributed practice</em>.
    </p>

    <p>
      Suppose you're introduced to someone at a party, and they tell
      you their name. If you're paying attention, and their name isn't
      too unusual, you'll almost certainly remember their name 20
      seconds later. But you're more likely to have forgotten their
      name in an hour, and more likely still to have forgotten their
      name in a month.
    </p>

    <p>
      That is, memories decay. This isn't news!  But the great German
      psychologist Hermann Ebbinghaus had the good idea of studying
      memory decay systematically and
      quantitatively*<span class="marginnote">* Hermann
      Ebbinghaus, <a href="http://psychclassics.yorku.ca/Ebbinghaus/index.htm">Memory:
      A Contribution to Experimental Psychology</a> (1885). A recent
      replication of Ebbinghaus's results may be found in: Jaap
      M. J. Murre and Joeri
      Dros, <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0120644">Replication
      and Analysis of Ebbinghaus' Forgetting Curve</a>
      (2015).</span>. In particular, he was interested in how quickly
      memories decay, and what causes the decay. To study this,
      Ebbinghaus memorized strings of nonsense syllables &ndash;
      things like &ldquo;fim&ldquo; and &ldquo;pes&rdquo; &ndash; and
      later tested himself, recording how well he retained those
      syllables after different time intervals.
    </p>

    <p>
      Ebbinghaus found that the probability of correctly recalling an
      item declined (roughly) exponentially with time.  Today, this is
      called the <em>Ebbinghaus forgetting curve</em>:
    </p>

    <img src="assets/Ebbinghaus.png">

    <p>
      What determines the steepness of the curve, i.e., how quickly
      memories decay? In fact, the steepness depends on many
      things. For instance, it may be steeper for more complex or less
      familiar concepts. You may find it easier to remember a name
      that sounds similar to names you've heard before: say, Richard
      Hamilton, rather than Suzuki Harunobu. So they'd have a
      shallower curve.  Similarly, you may find it easier to remember
      something visual than verbal. Or something verbal rather than a
      motor skill. And if you use more elaborate ways of remembering
      &ndash; mnemonics, for instance, or just taking care to connect
      an idea to other things you already know &ndash; you may be able
      to flatten the curve out*<span class="marginnote">* Although
      this expansion is much studied, there is surprisingly little
      work building detailed predictive models of the expansion.  An
      exception is: Burr Settles and Brendan
      Meeder, <a href="assets/Settles2016.pdf">A Trainable Spaced
      Repetition Model for Language Learning</a> (2016). This paper
      builds a regression model to predict the decay rate of student
      memory on Duolingo, the online language learning platform. The
      result was not only better prediction of decay rates, but also
      improved Duolingo student engagement.</span>.
    </p>

    <p>
      Suppose you're introduced to a person at a party, and then don't
      think about their name for 20 minutes. But then you need to
      introduce them to someone else, and so need to bring it to
      mind. Immediately after that, your probability of recall will
      again be very high.  Ebbinghaus's research suggested that the
      probability will decay exponentially after the re-test, but the
      rate of decay will be slower. In fact, subsequent re-tests slow
      the decay still more, a gradually flattening out of the decay
      curve as the memory is consolidated through multiple recall
      events:
    </p>

    <img src="assets/Ebbinghaus_repeat.png">

    <p>
      This gradual increase in decay time underlies the design of Anki
      and similar memory systems. It's why Anki gradually expands the
      time periods between testing.
    </p>

    <p>
      These phenomena are part of a broader set of ideas which have
      been extensively studied by scientists. There are several
      related terms used for this set of phenomena, but we'll use the
      phrase &ldquo;distributed practice&rdquo;, meaning practice
      which is distributed in time, ideally in a way designed to
      maximally promote retention.
    </p>

    <h3>On the role of cognitive science in the design of systems</h3>

      <p>
	Since Ebbinghaus, there's been thousands of studies of
	different variations of distributed practice. These studies
	have taught us a great deal about the behavior of long-term
	memory. It's tempting to jump into that literature, and to use
	it as a guide to the design of memory
	systems*<span class="marginnote">* Rather than do such a
	review, let me point to several reviews which serve as useful
	entry points. Benedict Carey's book &ldquo;How We Learn&rdquo;
	(2015) is a good introduction.  Useful reviews of distributed
	practice include: Cepeda <em>et
	al</em>, <a href="assets/Cepeda2006.pdf">Distributed Practice
	in Verbal Recall Tasks: A Review and Quantitative
	Synthesis</a> (2006); and: Gwern
	Branwen, <a href="https://www.gwern.net/Spaced-repetition">Spaced-Repetition</a>.</span>. But
	it's also worth thinking a little about the limitations of
	that literature as a guide to the development of systems.
      </p>

    <p>
      While scientists have done a tremendous number of studies of
      distributed practice, many very basic questions about
      distributed practice remain poorly understood.
    </p>

    <p>
      We don't understand in detail why exponential decay of memory
      occurs, or when that model breaks down. We don't have good
      models of what determines the rate of decay, and why it varies
      for different types of memories. We don't understand why the
      decay takes longer after subsequent recalls. And we have little
      understanding of the best way of expanding the inter-study
      intervals.
    </p>

    <p>
      Of course, there are many partial theories to answer these and
      other basic questions. But there's no single, powerful, broadly
      accepted general theory. And so in that sense, we know little
      about distributed practice, and are probably decades (if not
      more) away from a reasonably full understanding.
    </p>

    <p>
      To illustrate this point concretely, let me mention just one
      example: there are times when our memories don't decay, but get
      better over time, even when we're not aware of explicit acts of
      recall.  Informally, you have may have noticed this in your own
      life. The psychologist William James made the tongue-in-cheek
      observation, which he attributed to an unnamed German author,
      that*<span class="marginnote">* William James, &ldquo;The
      Principles of Psychology&rdquo; (1890).</span>
    </p>

    <blockquote>
      we learn to swim during the winter and to skate during the
      summer.
    </blockquote>

      <p>
	In fact, exactly such an effect was experimentally verified in
	an 1895 study of Axel Oehrn*<span class="marginnote">* Axel
	Oehrn, Experimentelle Studien zur Individualpsychologie
	(1895).</span>. While subsequent experiments have confirmed
	this result, it depends sensitively on the type of material
	being memorized, on the exact time intervals, and many other
	variables.  Now, in some sense this contradicts the Ebbinghaus
	exponential forgetting curve. In practice, a pretty good
	heuristic is that the Ebbinghaus curve holds approximately,
	but there are exceptions, usually over limited times, and for
	very specific types of materials.
      </p>

      <p>
	I don't mention this to undermine your belief in the
	Ebbinghaus model. But rather as a caution: memory is
	complicated, we don't understand many of the big picture
	questions well, and we should be careful before we put too
	much faith in any given model.
      </p>

      <p>
	With all that said: the basic effects underlying distributed
	practice are real, large, and have been confirmed by many, many
	experiments. Effects like that discovered by Oehrn are much less
	important by comparison.
      </p>

      <p>
	This places us in a curious situation: we have enough
	understanding of memory to conclude that a system like Anki
	should help a lot. But many of the choices needed in the
	design of such a system must be made in an
	<em>ad hoc</em> way, guided by intuition and unconfirmed
	hypotheses. The experiments in the scientific literature
	do <em>not</em> yet justify those design choices. The reason is
	that those experiments are mostly not intended to address those
	questions. They'll focus on specific types of information to
	memorize. Or they'll focus on relatively short periods of time
	&ndash; memorization over a day or a week, not for years. This
	doesn't mean such work isn't helping us build a better theory of
	memory. It just means it's not answering the questions designers
	need to build systems.
      </p>

      <p>
	As a consequence, system designers must look elsewhere, to
	informal experiments and theories. Anki, for example, uses a
	spacing algorithm developed by Piotr Wozniak on the basis of
	personal experimentation. Although Wozniak has published
	a <a href="https://www.supermemo.com/english/publicat.htm">number
	of papers</a>, they are informal reports, and don't abide by
	the norms of the conventional cognitive science literature.
      </p>

      <p>
	In some sense, this is not satisfactory: we don't understand
	what spacing schedule to use. But a system has to use some
	schedule, and so designers do the best they can. This seems
	likely to work much better than naive approaches, but over the
	long run it'd be good to have an approach based on a detailed
	theory of how human memory works.
    </p>

      <p>
	Now, one response to this is to say that you should design
	scientifically, and have good experimental evidence for all
	design choices. I've heard this used as a criticism of the
	designers of systems such as Anki, that they make too
	many <em>ad hoc</em> guesses, not backed by a systematic
	scientific understanding.
      </p>

      <p>
	But what are they supposed to do? Wait 50 or 100 years, until
	those answers are in? Give up design, and become memory
	scientists for the next 30 years, so they can give properly
	&ldquo;scientific&rdquo; answers to all the questions they
	need answered in the design of their systems?
      </p>

      <p>
	This isn't the way design works, nor the way it should work.
      </p>

      <p>
	If designers waited until all the evidence was in, no-one
	would ever design anything. In practice, what you want is
	imaginative, bold design, exploring many ideas, but inspired
	and informed (and not too constrained) by what is known
	scientifically. Ideally, alongside this there would be a much
	slower feedback loop, whereby design choices would suggest
	questions about memory, and scientific experiments, and the
	improved understanding of memory that resulted would suggest
	new avenues for design.
      </p>

      <p>
	This kind of balance is not easy to achieve.  The
	human-computer interaction community has tried to achieve
	it. But I don't think it's worked very
	well*<span class="marginnote">* I'm aware this comment won't
	make me any friends within the human-computer interaction
	community. On the other hand, I don't think it does any good
	to be silent about this. When I look at major events within
	the community, such as the CHI conference, the overwhelming
	majority of papers seem timid and with low aspirations
	compared to early work on augmentation. It's telling that
	publishing conventional papers (pdf, not even interactive
	JavaScript and HTML) is still so central to the
	field. </span>. It seems to me that they've given up a lot of
	boldness and imagination and aspiration in their design. At
	the same time, they're not doing full-fledged cognitive
	science either &ndash; they're not developing a detailed
	understanding of the mind. Getting the relationship between
	design and cognitive science right is a core problem. And it's
	not trivial to get right.
      </p>

      <!--
      <h3>The testing effect</h3>

      <p>
	In school, we usually think of &ldquo;studying&rdquo; and
	&ldquo;testing&rdquo; as two very different activities. Anki
	upends this: it makes testing into a form of studying; indeed,
	you're being tested over and over (and over!)  again. In fact,
	numerous studies have shown that being tested on material is a
	much more efficient way of learning material than is spending
	the same amount of time studying. This is known as
	the <em>testing effect</em>. Systems such as Anki which use
	the testing effect are said to involve <em>active recall</em>,
	as opposed to the <em>passive recall</em> that takes place
	when someone merely rereads material, rather than being
	actively tested on it.
    </p>

    <p>
      An influential demonstration of the testing effect is in a 2006
      paper by the psychologists Henry Roediger and Jeffrey
      Karpicke*<span class="marginnote">* Henry L. Roediger, III, and
      Jeffrey
      D. Karpicke, <a href="assets/Roediger2006a.pdf">Test-Enhanced
      Learning: Taking Memory Tests Improves Long-Term Retention</a>
      (2006)</span>. In one of their experiments, Roediger and
      Karpicke ask participants to study some simple essays. To do
      this, the participants were divided into three groups. One group
      read over (studied) the essays in four different sessions. We'll
      denote this group SSSS &ndash; study, study, study, study. A
      second group (SSST) read over the essays in the first three
      sessions, and then took a test (T) in the fourth session.  And a
      third group (STTT) read over the essays in the first session,
      and then took tests in the final three sessions. The length of
      the study and test sessions were arranged so that participants
      in all three groups engaged with the essay for the same total
      time.
    </p>

    <p>
      A week later, participants in all three groups were tested
      again, to see how well they could recall key ideas from the
      essays. The SSSS group recalled 40% of the ideas, the SSST group
      recalled about 56%, and the STTT group did best of all: 61%.
      Remarkably, this held even though participants got no feedback
      on how they'd done on intermediate tests. It seemed taking the
      tests promoted recall even without feedback, and repeated
      testing resulted in participants recalling about half as many
      items again as did repeated studying. (When experiments along
      similar lines are done <em>with</em> feedback, the testing
      effect usually continues to hold, and is sometimes stronger XXX
      check.)
    </p>

    <p>
      In another variation of their experiment, Roediger and Karpicke
      didn't wait a week to give the final test. Instead, they gave it
      just 5 minutes after the fourth session. In that experiment,
      participants in the SSSS group did best (83%), with the SSST
      group next (78%), and the STTT group worst (71%). In other
      words, over the short-term merely re-reading the essays was
      better than being tested. But repeated testing helped much more
      over the longer-term. Somehow it was promoting the formation of
      longer-term memories. XXX - something here bothers me.
    </p>

    <p>
      After completion of the four sessions, Roediger and Karpicke
      asked participants how well they thought they'd remember the
      material in a week.
    </p>

    <p>
      Can you guess what they found?
    </p>

    <p>
      In fact, people's self-perception was exactly the reverse of how
      the final tests actually turned out.  Members of the SSSS group
      thought they would recall much <em>better</em> than members of
      the SSST group, who in turn thought they'd recall somewhat
      better than members of the STTT group. Being tested made people
      less confident of their memory, but actually improved it more
      than re-reading.
    </p>

    <p>
      The testing effect has been widely studied. As with distributed
      practice, the picture that emerges has a lot of nuance. The
      strength of the testing effect can vary with the type of
      material, with the age of participants, with the type of test,
      and with many other variables.
    </p>

    <p>
      Still, the basic underlying phenomenon is clear: being tested on
      material usually produces significantly better memories.  What's
      more, although it hasn't been as thoroughly studied, there are
      also several studies suggesting participants feel as though they
      learn less from testing. It's as though while studying in the
      conventional way they're deluding themselves about how well
      things are going, while testing rudely interrupts those
      delusions &ndash; but ultimately produces longer-lasting
      memories.
    </p>

    <p>
      One challenge for systems based on active recall is that many
      people have principled objections to testing. I don't just mean
      the natural dislike that comes with the feeling of failure that
      comes when you don't know the answer to a question. I mean
      general arguments against the notion of testing itself, the kind
      of argument that have led many people to advocate for removing
      testing from schools.
    </p>

    <p>
      A common line of argument is that testing kills the joy of
      learning, turning something that should be its own intrinsic
      reward &ndash; learning &rdquo; into something motivated by
      extrinsic rewards, such as recognition for exceptional
      performance.
    </p>

    <p>
      I'm sympathetic to such arguments. But they don't apply to
      personal memory systems. You aren't doing Anki for an extrinsic
      reward. Rather, it's something you do for yourself, for an
      intrinsic reward: a better memory of things you're interested
      in.
    </p>

    <p>
      And so I believe many common arguments against testing don't
      apply to personal memory systems such as Anki. Of course, such
      arguments might apply if such systems were imposed compulsorily
      in the classroom. But that's more a statement about classroom
      culture than it is anything to do with personal memory systems.
    </p>

    -->
      
    <h3>Who should build tools to augment human cognition?</h3>
    

    <p>
      In 2014, <em>The New York Times</em> science journalist Benedict
      Carey published the book &ldquo;How We Learn&rdquo;. It's a
      popular science book, with extended discussion of distributed
      practice and many related ideas.  Carey frames the book as a
      kind of self-help, bringing you up-to-date on cognitive science
      research, with the goal of helping you be more effective in your
      learning.
    </p>

    <p>
      It's a good book, and an excellent, thoughtful review of some of
      the main ideas in the cognitive science of
      learning. Unfortunately, Carey has little to say about how to
      take advantage of these ideas. It's one thing to understand that
      distributed practice is much better than cramming. It's quite
      another thing to actually systematically act on this
      understanding. Indeed, Carey's and other discussions of
      distributed practice often frame it as though people are making
      a choice between cramming and distributed practice. But that's
      not right. For all but the most organized people, it's not so
      much that you're choosing between cramming and distributed
      practice. Rather, you're choosing between cramming and no study
      at all.
    </p>

    <p>
      These points apply more broadly to the entire cognitive science
      literature. It's one thing to know how memory works. It's quite
      another to develop a really good, easily usable system that
      makes it possible to take advantage of that understanding of
      memory. Developing such systems is a difficult skill, a skill
      very different to studying memory.
    </p>

    <p>
      And so it is only natural that the systems that enable us to act
      on the insights of cognitive scientists have been built by
      systems designers*<span class="marginnote">* I'm mostly going to
      use the term &ldquo;designers&rdquo;, although in many cases the
      designer is also a programmer who actually builds the
      system.</span>, not by cognitive scientists.
    </p>

    <p>
      Anki, for example, was developed by Damien Elmes, a professional
      programmer who works fulltime on the project.
    </p>

    <p>
      Anki is a descendant of the
      program <a href="https://www.supermemo.com/en/frontpage">SuperMemo</a>,
      developed by a researcher named Piotr Wozniak who has devoted
      much of his life to developing systems for spaced repetition. In
      some sense, Wozniak is a combined researcher and designer. But
      his research is not at all research in the conventional sense of
      cognitive science. Rather, it's devoted almost entirely to
      improving SuperMemo and related systems. Looking through
      his <a href="http://super-memory.com/english/publicat.htm">publications</a>
      it's striking how different they are from typical papers in
      cognitive science.
    </p>

    <p>
      Prior to SuperMemo, the science writer Sebastian Leitner
      developed a card-based system for learning foreign languages. It
      was not computerized, and somewhat cumbersome, requiring the
      user to manually move cards from one part of the filing system
      to another. Still, it became extremely popular in Germany.
    </p>

    <p>
      None of these people are cognitive scientists in the
      conventional sense. Rather, they are system designers and
      builders.
    </p>

    <p>
      In general, it's notable how little direct impact cognitive
      scientists have had on how people think. This is because
      cognitive science is about studying how people actually think,
      not (for the most part) prescribing how they should think, or
      about building systems to help them think more effectively. This
      is not a criticism of cognitive science. Rather, it's just an
      acknowledgment that the norms of cognitive science do not
      favor cognitive scientists building or studying systems.
    </p>

    <p>
      When I point this out, sometimes people suggest that if only the
      norms of cognitive science were to change, cognitive scientists
      would start building systems. This underestimates just how
      difficult systems design and building is. Someone with the
      skills to build a magnificent theory of human memory may know
      very little about how to build systems that will enable memory
      to operate at its best. That's because building effective
      systems &ndash; even prototype systems &ndash; is very, very
      hard work to do well. We'll come back to this line of
      questioning in Chapter XXX.
    </p>

    <p>
      This is, in part, why I believe we need a field of human
      augmentation. That field will take input from cognitive
      science. But it will fundamentally be a design science, oriented
      toward asking questions about what kind of systems we can build,
      and then building, all the way from prototype to large-scale
      deployment.
    </p>


    <a name="Anki_analysis"></a>
    <h2>Appendix 1: analysis of Anki study time</h2>
      
    <p>
      Here's a ballpark analysis of the effort required to study an
      Anki card for recall over 20 years &ndash; what we might
      reasonably consider lifetime recall. Note that the analysis is
      sensitive to the detailed assumptions made, so the time
      estimates shouldn't be taken too seriously. Nonetheless, it's
      useful to get a sense of the times involved.
    </p>

    <p>
      When a card is initially entered, Anki requires reviews after
      just 1 minute and then 10 minutes. After those reviews the
      interval between reviews rises substantially, to 1 day. The
      interval expansion rate after that may vary a
      little*<span class="marginnote">* The reason is that Anki allows
      you to specify that you found a card &ldquo;easy&rdquo; or
      &ldquo;hard&rdquo; when you review it, in addition to the
      generic &ldquo;good&rdquo; (meaning you got it right) or
      &ldquo;again&rdquo; (meaning you got it wrong). Those additional
      options vary the exact rate of interval expansion. In practice,
      I nearly always choose &ldquo;good&rdquo;, or tell Anki that I
      got the card wrong.</span>, but for my cards the typical
      expansion rate is by a factor of about 2.4 for each successful
      review. That means that successful reviews will raise the
      interval to 2.4 days, then to 2.4 * 2.4 = 6.76 days, and so on.
      On average, I get about 1 in 12 cards wrong, so by the 12th card
      we're up to about 2.4<sup>9</sup> = 2,642 days between
      reviews. Note that we raise to the 9<sup>th</sup> power rather
      than the 12<sup>th</sup> power, because it's not until the third
      repetition of a card that the interval reaches 1 day.
    </p>

    <p>
      If you sum those intervals all up, it suggests the typical time
      between failed reviews is about 12 years.  Note, however, that I
      haven't been using Anki for nearly that long, and this estimate
      may be over-optimistic.  We can get a lower bound on the time
      between failures by observing that my mean interval between card
      reviews is already 1.2 years.  To achieve an interval of 1.2
      years requires about 0.9 years of successful prior reviews, so
      on average my cards involve at least 2.1 years between
      failures. However, the real number may be much higher, since
      there's no reason to assume my next review on most of those
      cards is going to fail.  So let's say that a conservative
      estimate is a mean time between failures of between 4 and 7
      years.
    </p>

    <p>
      If we assume the mean time between failures is 4 years, then
      over 20 years that means 5 failures, and reviewing 5 failures *
      10 reviews per period = 50 times, for a total of 50 * 8 seconds
      = 400 seconds, or about 7 minutes.
    </p>

    <p>
      If instead we assume the mean time between failures is 7 years,
      then over 20 years that means roughly 3 failures, and reviewing
      3 failures * 11 reviews per period = 33 times, for a total of 33
      * 8 seconds &approx; 260 seconds, or about 4 minutes.
    </p>

    <p>
      Note that in Anki's model a failure resets the review interval
      back to 10 minutes, then to 1 day, 2.4 days, and so on. In
      practice, that seems much too conservative.  After one or two
      failures with a card I usually catch on, and it would be better
      if Anki wasn't so draconian in resetting the review schedule. A
      better review schedule would reduce the total study time, and I
      wouldn't be surprised if a commitment of &tilde;2 minutes was
      closer to typical, for an average card.
    </p>

    <a name="Anki_APIs"></a>
    <h2>Appendix 2: Using Anki to learn APIs</h2>
    
    <p>
      Another good use for Anki is to assist in learning APIs.  Here's
      some patterns which work for me, and a few warnings about
      anti-patterns.
    </p>

    <p>
      It begins with me deciding there's some API I'd like to learn to
      use in a project. Now, some of the time, I just want to use the
      API a little &ndash; say, for 50-100 lines of code, or even just
      some 1-10 line code snippets. In that case I'm best off winging
      it, adapting snippets from elsewhere, and consulting the docs as
      needed.
    </p>

    <p>
      But suppose I know I will use the API more seriously in a
      project.  For instance, for my
      essay <a href="http://cognitivemedium.com/tat/">Thought as a
      Technology</a> I wanted to build some prototypes using 3d
      graphics, and decided to learn the basics of
      the <a href="https://threejs.org/">three.js</a> Javascript
      library.
    </p>

    <p>
      One tempting failure mode is to think &ldquo;Oh, I should master
      the API first&rdquo;, and then to dive into tutorials or the
      documentation. Apart from a quick skim of a tutorial or the
      documentation, that's a mistake. A better approach is to find a
      small, functioning piece of code that does something related to
      the core functionality of my project. It doesn't need to be
      similar to the whole project, but ideally implements one or two
      similar features, and is a few tens or hundreds of lines of code
      long. I get that code running, then start making small tweaks,
      adding bits of functionality I need, taking out bits that I
      don't, and trying to understand and improve the code.
    </p>

    <span class="quote_right">
      I probably err on the side of just making things
      happen&hellip; I get so much of a thrill bringing things to
      life&hellip; as soon as it comes to life it starts telling
      you what it is. - Dan Ingalls
    </span>
    
    <p>
      The great thing about this is that I need only change 1 to 5
      lines of code at a time, and I see meaningful progress toward my
      goals. That's exciting. To use a metaphor from machine learning,
      it's like doing gradient descent in the space of meaningful
      projects.
    </p>

    <p>
      Of course, while doing this, I'll constantly be looking up
      things in the docs, on StackOverflow, and so on. I'll also be
      reading and understanding pieces of the code I started
      from. It's tempting to Ankify all this, but it's a mistake: it
      takes too much time, and you Ankify too much that later turns
      out to be little use.  However, when something is clearly a
      central concept, or I know I'll reuse it often, it's worth
      adding to Anki. In this way, I gradually build up a knowledge
      base of things I can use in real, live projects. And, slowly, I
      get better and better.
    </p>

    <p>
      Once I'm making real progress on my project, and confident I've
      made a good choice of API, then it makes sense to work through a
      tutorial. I usually dip quickly into several such tutorials, and
      identify the one I believe I can learn most quickly from. And
      then I work through it. I do Ankify at this stage, but keep it
      relatively light. It's tempting to Ankify everything, but I end
      up memorizing lots of useless information, at great time
      cost. It's much better to only Ankify material I know I'll need
      repeatedly. Usually that means I can already see I need it right
      now, at the current stage of my project. On the first pass, I'm
      conservative, Ankifying less material. Then, once I've gone
      through a tutorial once, I go back over it, this time Ankifying
      everything I'm likely to need later. This second pass is usually
      quite rapid &ndash; often faster than the first pass &ndash; but
      on the second pass I have more context, and my judgment about
      what to Ankify is better.
    </p>

    <p>
      I continue doing this, bouncing back and forth between working
      on my project and working on Anki as I make my way through
      tutorials and documentation, as well as material that comes up
      while reading code &ndash; code from others, and even code I've
      written myself. I also find it helpful to sometimes Ankify the
      APIs for code I've personally written, if they're likely to be
      useful in the future. Just because I wrote something doesn't
      mean I'll automatically remember it in future!
    </p>
    
    <p>
      So: don't jump into Ankifying tutorials and documentation
      straight away. Wait, and do it in tandem with serious work on
      your project. I must admit, part of the reason I advise this is
      because I find the advice hard to take myself, and I always
      regret not following it. I start a new project, think &ldquo;Oh,
      I need such-and-such an API&rdquo;, and then dive into a
      tutorial, spending hours on it. But I struggle and struggle and
      make very slow progress. Until I remember to find some working
      code to start from, and immediately find things are going much
      better. I then swear to never use the tutorial-first approach
      again. Unfortunately, in practice, I find it seductive.
    </p>

      <p>
	The overall process is much like the common learning-by-doing
	approach to a new API, where you gradually learn the API
	through repetition, while working on a project. The main
	difference is that the occasional interspersed use of Anki
	considerably speeds up the rate at which you agglomerate new
	knowledge.
      </p>
      
    <p>
      A potential failure mode is to think &ldquo;Oh, I might want to
      learn such-and-such an API one day, so I should start adding
      cards, even though I don't currently have a project where I'm
      using the API.&rdquo;
    </p>

    <p>
      I've tried this a couple of times, and my advice is: don't do
      it.
    </p>

    <p>
      It's a form of a problem I described in the main body of the
      essay: the temptation to stockpile knowledge against some day
      when you'll use it. You will learn far more quickly if you're
      simultaneously using the API seriously in a project.  Using the
      API to create something new helps you identify what is important
      to remember from the API. And it also &ndash; this is
      speculation &ndash; sends a signal to your brain saying
      &ldquo;this really matters&rdquo;, and that helps your memory
      quite a bit. So if you're tempted to do highly speculative
      Ankification, please don't. And if you find yourself starting,
      stop.
    </p>

    <p>
      A more challenging partial failure mode is Ankifying what turn
      into orphan APIs. That is, I'll use a new API for a project, and
      Ankify some material from the API. Then the project finishes,
      and I don't immediately have another project using the same
      API. I then find my mind won't engage so well with the cards
      &ndash; there's a half-conscious thought of &ldquo;why am I
      learning this useless stuff?&rdquo; I just no longer find the
      cards as interesting as when I was actively using the API.
    </p>

    <p>
      This is a difficult situation. I use the rule of thumb that if
      it seems likely I'm not going to use the API again, I delete the
      cards when they come up. But if it seems likely I'll use the API
      in the next year or so, I keep them in the deck. It's not a
      perfect solution, since I really do slightly disconnect from the
      cards. But it's the best compromise I've found.
    </p>

    <h3>Acknowledgments</h3>

    <p>
      I became intrigued with Anki in large part due to the writing
      of <a href="https://www.gwern.net/Spaced-repetition">Gwern
      Branwen</a>, <a href="https://sasha.wtf/anki-post-1/amp/">Sasha
      Laundy</a>, and <a href="https://sivers.org/srs">Derek
      Sivers</a>.  Thanks to Mason Hartman, Andy Matuschak, and Kevin
      Simler for many useful conversations about this essay. I'm
      particularly grateful to Andy Matuschak for many detailed
      conversations, and especially for pointing out how unusual is my
      view that Anki use is a virtuoso skill that can be developed.
    </p>
    
      testing 


    <script src="assets/adjust_marginnotes.js" type="text/javascript">
    </script>
    
  </body>
</html>
